{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import Necessary Libraries:\n",
    "Import required libraries like pandas for data manipulation, re for regular expressions, and various modules from scikit-learn for machine learning tasks, including data preprocessing, feature extraction, and model evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import re\n",
    "import pickle\n",
    "from sklearn.utils import resample\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, TfidfTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, classification_report, confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load the Training and Testing Data:\n",
    "Use pandas to load your training and testing data from CSV files. This data will be used for training and testing your hate speech detection model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the training and testing data\n",
    "train_data = pd.read_csv('train.csv')\n",
    "test_data = pd.read_csv('test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Text Cleaning Function (clean_text):\n",
    "This function cleans the text data by performing the following operations on input text:\n",
    "\n",
    "* Convert text to lowercase.\n",
    "* Remove special characters and punctuation marks.\n",
    "* Remove numbers.\n",
    "* Remove URLs.\n",
    "* Remove hashtags.\n",
    "\n",
    "The function takes an input text (or a pandas Series) and returns the cleaned text (or a cleaned Series)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    \"\"\"\n",
    "    Cleans text data by converting to lowercase, removing special characters, numbers, URLs, and hashtags.\n",
    "    \n",
    "    Args:\n",
    "        text (str or pd.Series): Input text or text Series to be cleaned.\n",
    "    \n",
    "    Returns:\n",
    "        str or pd.Series: Cleaned text or text Series.\n",
    "    \"\"\"\n",
    "    if isinstance(text, str):\n",
    "        text = text.lower()\n",
    "        text = re.sub(r'[^\\w\\s]', '', text, re.UNICODE)\n",
    "        text = re.sub(r'[0-9]+', '', text)\n",
    "        text = re.sub(r'(@[^a-zA-Z]+)|(\\\\w+:\\\\/\\\\/\\\\s+)|^rt|http.+?', ' ', text)\n",
    "        text = re.sub(r'#\\w*', '', text)\n",
    "        return text\n",
    "    elif isinstance(text, pd.Series):\n",
    "        cleaned_series = text.str.lower()\n",
    "        cleaned_series = cleaned_series.str.replace(r'[^\\w\\s]', '', regex=True)\n",
    "        cleaned_series = cleaned_series.str.replace(r'[0-9]+', '', regex=True)\n",
    "        cleaned_series = cleaned_series.str.replace(r'(@[^a-zA-Z]+)|(\\\\w+:\\\\/\\\\/\\\\s+)|^rt|http.+?', ' ', regex=True)\n",
    "        cleaned_series = cleaned_series.str.replace(r'#\\w*', '', regex=True)\n",
    "        return cleaned_series\n",
    "    else:\n",
    "        raise ValueError(\"Input must be a string or a pandas Series.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Apply Text Cleaning to Training and Testing Data:\n",
    "Use the clean_text function to clean the tweet text in both the training and testing datasets. This cleaning process prepares the text data for feature extraction and modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply text cleaning to the training and testing data\n",
    "\n",
    "train_data['tweet'] = train_data['tweet'].apply(lambda x: clean_text(x))\n",
    "test_data['tweet'] = test_data['tweet'].apply(lambda x: clean_text(x))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Upsample the Minority Class:\n",
    "Address the class imbalance issue by oversampling the minority class (label 1) in the training data using the resample function from scikit-learn. The minority class is randomly resampled with replacement to match the majority class size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upsample the minority class\n",
    "\n",
    "train_majority = train_data[train_data.label == 0]\n",
    "train_minority = train_data[train_data.label == 1]\n",
    "train_minority_upsampled = resample(train_minority, replace=True, n_samples=29720, random_state=123)\n",
    "train_upsampled = pd.concat([train_majority, train_minority_upsampled])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Split the Data into Training and Testing Sets:\n",
    "Split the upsampled training data into training and testing sets using train_test_split. The split will ensure that you have training and testing data for model training and evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and testing sets\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(train_upsampled['tweet'], train_upsampled['label'], random_state=0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Machine Learning Pipeline:\n",
    "Define a machine learning pipeline that includes the following steps:\n",
    "\n",
    "* list_to_series: A custom transformation step to convert the data into a pandas Series format for compatibility with other pipeline components.\n",
    "* CountVectorizer: Convert the text data into a matrix of token counts (bag of words).\n",
    "* TfidfTransformer: Apply TF-IDF (Term Frequency-Inverse Document Frequency) transformation to the token counts.\n",
    "* RandomForestClassifier: Use a Random Forest Classifier with 100 decision trees for text classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a machine learning pipeline\n",
    "\n",
    "# Define a function to convert a list to a pandas Series\n",
    "def list_to_series(data):\n",
    "    return pd.Series(data)\n",
    "\n",
    "# Define the pipeline\n",
    "pipeline = Pipeline([\n",
    "    ('list_to_series', FunctionTransformer(list_to_series, validate=False)),\n",
    "    ('vect', CountVectorizer()),\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "    ('clf', RandomForestClassifier(n_estimators=100, random_state=0)),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train the Model:\n",
    "Fit the machine learning pipeline (model) to the training data (X_train and y_train). The model combines the feature extraction and classifier into one pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "\n",
    "model = pipeline.fit(X_train, y_train)\n",
    "y_predict = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Store the Model:\n",
    "Save the trained model to a file using the pickle library. The saved model can be loaded and used later for predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store the model\n",
    "\n",
    "model_filename = 'finalized_model.sav'\n",
    "pickle.dump(model, open(model_filename, 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluate the Model:\n",
    "Calculate various performance metrics for the model, including accuracy, precision, recall, F1 score, classification report, and the confusion matrix, using y_test and the model's predictions (y_predict). These metrics help assess the model's performance in detecting hate speech."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_predict)\n",
    "precision = precision_score(y_test, y_predict)\n",
    "recall = recall_score(y_test, y_predict)\n",
    "f1 = f1_score(y_test, y_predict)\n",
    "classification_rep = classification_report(y_test, y_predict)\n",
    "confusion_mtx = confusion_matrix(y_test, y_predict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Print Evaluation Results:\n",
    "Display the evaluation results, including accuracy, precision, recall, F1 score, the classification report (with precision, recall, and F1 for each class), and the confusion matrix. These results provide insights into the model's strengths and weaknesses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score:  0.9985868102288021\n",
      "Precision Score:  0.9972041006523765\n",
      "Recall Score:  1.0\n",
      "F1 Score:  0.9986000933271115\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      7370\n",
      "           1       1.00      1.00      1.00      7490\n",
      "\n",
      "    accuracy                           1.00     14860\n",
      "   macro avg       1.00      1.00      1.00     14860\n",
      "weighted avg       1.00      1.00      1.00     14860\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      " [[7349   21]\n",
      " [   0 7490]]\n"
     ]
    }
   ],
   "source": [
    "# Print evaluation results\n",
    "\n",
    "print('Accuracy Score: ', accuracy)\n",
    "print('Precision Score: ', precision)\n",
    "print('Recall Score: ', recall)\n",
    "print('F1 Score: ', f1)\n",
    "print('\\nClassification Report:\\n', classification_rep)\n",
    "print('\\nConfusion Matrix:\\n', confusion_mtx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load the Model:\n",
    "Use pickle to load the previously stored model (finalized_model.sav) from the file. This allows you to make predictions using the trained model without retraining it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model\n",
    "\n",
    "loaded_model = pickle.load(open(model_filename, 'rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use the Model:\n",
    "Finally, demonstrate how to use the loaded model for predictions. In this case, you make predictions on a single test tweet from the testing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\n"
     ]
    }
   ],
   "source": [
    "# Use the model\n",
    "\n",
    "result = loaded_model.predict([test_data['tweet'][4]])\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "\n",
    "Overall, your code is a well-structured pipeline for hate speech detection, with attention to data preprocessing, \n",
    "resampling, model training, evaluation, and model persistence. It provides a clear and organized workflow for building \n",
    "and assessing hate speech detection models. The code is also well-documented and easy to follow.\n",
    "\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML_BOOK_ENV",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
